{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# experiment for paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from collections import defaultdict\n",
    "import subprocess\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build Tool Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folders = os.listdir(\"../extractor/apidocs\")\n",
    "parameter_dict = defaultdict(set)\n",
    "def add_to_dict(d: defaultdict, prev_path: str, object, api_doc: str):\n",
    "    '''encode a json tree into a dictionary with root to leaf path. sets have max length of 10'''\n",
    "    if isinstance(object, dict):\n",
    "        for key, value in object.items():\n",
    "            add_to_dict(d, prev_path + \"[\" + key + \"]\", value, api_doc)\n",
    "    elif isinstance(object, list):\n",
    "        for item in object:\n",
    "            add_to_dict(d, prev_path, item, api_doc)\n",
    "    else:\n",
    "        if object:\n",
    "            if len(d[prev_path]) < 10:\n",
    "                d[prev_path].add((api_doc, object))\n",
    "def build_dict(d, object, api_doc):\n",
    "    add_to_dict(d, '', object, api_doc)\n",
    "for folder in folders:\n",
    "    path = \"../extractor/apidocs/\" + folder + \"/\" + folder + \".txt\"\n",
    "    try:\n",
    "        json_file = json.load(open(path))\n",
    "    except:\n",
    "        continue\n",
    "    for endpoint in json_file['endpoints']:\n",
    "        for param in endpoint['required_parameters']:\n",
    "            name = param['name']\n",
    "            example = None if not param['example'] else param['example']\n",
    "            add_to_dict(parameter_dict, \"[\"+name+\"]\", example, folder)\n",
    "            \n",
    "        if endpoint['optional_parameters']:\n",
    "            for param in endpoint['optional_parameters']:\n",
    "                name = param['name']\n",
    "                example = None if not param['example'] else param['example']\n",
    "                add_to_dict(parameter_dict, \"[\"+name+\"]\", example, folder)\n",
    "parameter_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folders = os.listdir(\"../extractor/apidocs\")\n",
    "description_to_param_dict = {} # map description to parameter\n",
    "param_to_description_dict = {} # map parameter to description\n",
    "for folder in folders:\n",
    "    path = \"../extractor/apidocs/\" + folder + \"/\" + folder + \".txt\"\n",
    "    try:\n",
    "        json_file = json.load(open(path))\n",
    "    except:\n",
    "        continue\n",
    "    for endpoint in json_file['endpoints']:\n",
    "        for param in endpoint['required_parameters']:\n",
    "            name = param['name']\n",
    "            description = param['description']\n",
    "            description_to_param_dict[description] = name\n",
    "            param_to_description_dict[name] = description\n",
    "        if endpoint['optional_parameters']:\n",
    "            for param in endpoint['optional_parameters']:\n",
    "                name = param['name']\n",
    "                description = param['description']\n",
    "                description_to_param_dict[description] = name\n",
    "                param_to_description_dict[name] = description\n",
    "\n",
    "description_to_param_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get API response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# no need to run again\n",
    "apidocs_dir = \"../extractor/apidocs\"\n",
    "count = 0\n",
    "evaluation_result={}\n",
    "folders = os.listdir(apidocs_dir)\n",
    "for folder in folders:\n",
    "    files = os.listdir(os.path.join(apidocs_dir, folder))\n",
    "    failed_endpoints = []\n",
    "    api_result={}\n",
    "    files = [x for x in files if x.endswith(\".py\")]\n",
    "    for file_name in files:\n",
    "        if file_name.endswith(\".py\"):\n",
    "            count += 1\n",
    "            file_path = os.path.join(apidocs_dir, folder, file_name)\n",
    "            try:\n",
    "                result = subprocess.run(\n",
    "                    [\"python\", file_path], capture_output=True, text=True, check=True\n",
    "                )\n",
    "                if result.stdout:\n",
    "                    output = result.stdout\n",
    "                    output_json = json.loads(output)\n",
    "                    # save in file\n",
    "                    with open(file_path[:-3] + '_response.json', \"w\") as f:\n",
    "                        json.dump(output_json, f)\n",
    "                else:\n",
    "                    pass\n",
    "            except subprocess.CalledProcessError as e:\n",
    "                pass\n",
    "            print(f\"Tested: {file_name}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "apidocs_dir = \"../extractor/apidocs\"\n",
    "count = 0\n",
    "evaluation_result={}\n",
    "folders = os.listdir(apidocs_dir)\n",
    "response_dict = defaultdict(set)\n",
    "for folder in folders:\n",
    "    files = os.listdir(os.path.join(apidocs_dir, folder))\n",
    "    failed_endpoints = []\n",
    "    api_result={}\n",
    "    files = [x for x in files if x.endswith(\"_response.json\")]\n",
    "    for file_name in files:\n",
    "        file_path = os.path.join(apidocs_dir, folder, file_name)\n",
    "        response = json.load(open(file_path))\n",
    "        if response['status_code'] == 200:\n",
    "            response_json = response['json']\n",
    "            build_dict(response_dict, response_json, folder)\n",
    "response_dict\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate embedding, build parameter knowledge base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from charset_normalizer import md__mypyc\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "model = SentenceTransformer(\"flax-sentence-embeddings/st-codesearch-distilroberta-base\")\n",
    "param_keys = list(parameter_dict.keys())\n",
    "param_keys_emb = model.encode(param_keys, convert_to_tensor=True)\n",
    "response_keys = list(response_dict.keys())\n",
    "response_keys_emb = model.encode(response_keys, convert_to_tensor=True)\n",
    "description_keys = list(description_to_param_dict.keys())\n",
    "description_keys_emb = model.encode(description_keys, convert_to_tensor=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = 'iupacextended'\n",
    "query_emb = model.encode(query, convert_to_tensor=True) \n",
    "hits_param = util.semantic_search(query_emb, param_keys_emb)\n",
    "hits_response = util.semantic_search(query_emb, response_keys_emb)\n",
    "query_description = param_to_description_dict[query]\n",
    "query_description_emb = model.encode(query_description, convert_to_tensor=True)\n",
    "hits_description = util.semantic_search(query_description_emb, description_keys_emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h = hits_param[0]\n",
    "for hit in h:\n",
    "    print(f\"Param: {param_keys[hit['corpus_id']]} Example: {parameter_dict[param_keys[hit['corpus_id']]]}\")\n",
    "    print(f\"Similarity: {hit['score']}\")\n",
    "    print(\"\")\n",
    "\n",
    "print(\"==================================\")\n",
    "h = hits_response[0]\n",
    "for hit in h:\n",
    "    print(f\"Request: {response_keys[hit['corpus_id']]} Example: {response_dict[response_keys[hit['corpus_id']]]}\")\n",
    "    print(f\"Similarity: {hit['score']}\")\n",
    "    print(\"\")\n",
    "\n",
    "print(\"==================================\")\n",
    "h = hits_description[0]\n",
    "for hit in h:\n",
    "    print(f\"Description: {description_keys[hit['corpus_id']]} Example: {parameter_dict['['+description_to_param_dict[description_keys[hit['corpus_id']]]+']']}\")\n",
    "    print(f\"Similarity: {hit['score']}\")\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load generated tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "def load_and_import(api_name, function_name):\n",
    "    folder_path = os.path.join(\"..\",\"extractor\", \"apidocs\", api_name)\n",
    "    sys.path.insert(0, folder_path)\n",
    "    module = __import__(function_name)\n",
    "    sys.path.pop(0)  # Clean up after import\n",
    "    return module\n",
    "\n",
    "module = load_and_import(\"glycanformatconverter\", \"convert_glycoct_to_wurcs_GET\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = getattr(module, \"convert_glycoct_to_wurcs\")\n",
    "response = f(glycoct='''RES\\n1b:x-dglc-HEX-1:5\\n2s:n-acetyl\\n3b:b-dglc-HEX-1:5\\n4s:n-acetyl\\n5b:b-dman-HEX-1:5\\n6b:a-dman-HEX-1:5\\n7b:a-dman-HEX-1:5\\nLIN\\n1:1d(2+1)2n\\n2:1o(4+1)3d\\n3:3d(2+1)4n\\n4:3o(4+1)5d\\n5:5o(3+1)6d\\n6:5o(6+1)7d''')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# query KB and get parameter value candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_function(api_name, endpoint_name):\n",
    "    function_name = endpoint_name.replace(\"_GET\", \"\").replace(\"_POST\", \"\")\n",
    "    module = load_and_import(api_name, endpoint_name)\n",
    "    f = getattr(module, function_name)\n",
    "    return f\n",
    "\n",
    "import random\n",
    "def get_test_examples(query, api_doc_name, max_param_examples=5, max_response_examples=5, max_examples_per_hit=1, similarity_threshold=0.5, query_description=None):\n",
    "    if not query_description:\n",
    "        try:\n",
    "            query_description = param_to_description_dict[query]\n",
    "        except:\n",
    "            query_description = None\n",
    "    query_emb = model.encode(query, convert_to_tensor=True) \n",
    "    hits_param = util.semantic_search(query_emb, param_keys_emb)\n",
    "    hits_response = util.semantic_search(query_emb, response_keys_emb)\n",
    "    if query_description:\n",
    "        query_description_emb = model.encode(query_description, convert_to_tensor=True)\n",
    "        hits_description = util.semantic_search(query_description_emb, description_keys_emb)\n",
    "    hit_list = hits_param[0]\n",
    "    all_examples = set()\n",
    "    count = 0\n",
    "    for hit in hit_list:\n",
    "        if hit['score'] < similarity_threshold or count > max_param_examples:\n",
    "            break\n",
    "        param_key = param_keys[hit['corpus_id']]\n",
    "        examples_set = parameter_dict[param_key]\n",
    "        filted_examples = [t[1] for t in examples_set if t[0]!= api_doc_name]\n",
    "        # print([(t[0],t[1]) for t in examples_set if t[0]!= api_doc_name])\n",
    "        examples = random.sample(filted_examples, min(len(filted_examples), max_examples_per_hit))\n",
    "        sidx = param_key.rfind('[')+1\n",
    "        test_param = param_key[sidx:-1]\n",
    "        # print(test_param, examples)\n",
    "        pairs = [(test_param, example) for example in examples]\n",
    "        all_examples.update(pairs)\n",
    "        # print(all_examples)\n",
    "        count += len(examples)\n",
    "    hit_list = hits_response[0]\n",
    "    count = 0\n",
    "    for hit in hit_list:\n",
    "        if hit['score'] < similarity_threshold or count > max_response_examples:\n",
    "            break\n",
    "        response_key = response_keys[hit['corpus_id']]\n",
    "        examples_set = response_dict[response_key]\n",
    "        examples_set_copy = examples_set.copy()\n",
    "        filted_examples = [t[1] for t in examples_set if t[0]!= api_doc_name]\n",
    "        # print([(t[0],t[1]) for t in examples_set if t[0]!= api_doc_name])\n",
    "        examples = random.sample(filted_examples, min(len(filted_examples), max_examples_per_hit))\n",
    "        sidx = response_key.rfind('[')+1\n",
    "        test_param = response_key[sidx:-1]\n",
    "        # print(test_param, examples)\n",
    "        pairs = [(test_param, example) for example in examples]\n",
    "        all_examples.update(pairs)\n",
    "        # print(all_examples)\n",
    "        count += len(examples)\n",
    "    if query_description:\n",
    "        hit_list = hits_description[0]\n",
    "        count = 0\n",
    "        for hit in hit_list:\n",
    "            if hit['score'] < similarity_threshold or count > max_param_examples:\n",
    "                break\n",
    "            description_key = description_keys[hit['corpus_id']]\n",
    "            param_key = description_to_param_dict[description_key]\n",
    "            examples_set = parameter_dict['['+param_key+']']\n",
    "            filted_examples = [t[1] for t in examples_set if t[0]!= api_doc_name]\n",
    "            # print([(t[0],t[1]) for t in examples_set if t[0]!= api_doc_name])\n",
    "            examples = random.sample(filted_examples, min(len(filted_examples), max_examples_per_hit))\n",
    "            # print(param_key, examples)\n",
    "            pairs = [(param_key, example) for example in examples]\n",
    "            all_examples.update(pairs)\n",
    "            # print(all_examples)\n",
    "            count += len(examples)\n",
    "    return all_examples\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_test_examples(\"iupacextended\", \"glycanformatconverter\", max_param_examples=5, max_response_examples=5, max_examples_per_hit=1, similarity_threshold=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import inspect\n",
    "import time\n",
    "import itertools\n",
    "def get_parameter_names(func):\n",
    "    signature = inspect.signature(func)\n",
    "    return [param.name for param in signature.parameters.values()]\n",
    "def run_experiment(MAX_SAMPLES=100):\n",
    "    apidocs_dir = \"../extractor/apidocs\"\n",
    "    folders = os.listdir(apidocs_dir)\n",
    "    for folder in folders:\n",
    "        files = os.listdir(os.path.join(apidocs_dir, folder))\n",
    "        files = [x for x in files if x.endswith(\".py\")]\n",
    "        api_extracted_json = {}\n",
    "        with open(os.path.join(apidocs_dir, folder, folder + \".txt\")) as f:\n",
    "            api_extracted_json = json.load(f)\n",
    "        extracted_endpoints = None\n",
    "        try:\n",
    "            extracted_endpoints = api_extracted_json['endpoints']\n",
    "        except:\n",
    "            print(\"No extracted information for api: \", folder)\n",
    "            extracted_endpoints = None\n",
    "            continue\n",
    "        for idx, file_name in enumerate(files):\n",
    "            endpoint_name = file_name.replace(\".py\", \"\")\n",
    "            record_file_name = endpoint_name + \"_example_test.json\"\n",
    "            if os.path.exists(os.path.join(apidocs_dir, folder, record_file_name)):\n",
    "                endpoint_result = json.load(open(os.path.join(apidocs_dir, folder, record_file_name)))\n",
    "            else:\n",
    "                endpoint_result = {\"endpoint\": endpoint_name, \"tests\": [], \"extracted_parameters\": {}, \"validated_parameters\": {}}\n",
    "                print(f\"Testing {folder}.{endpoint_name}\")\n",
    "                if file_name == \"__init__.py\":\n",
    "                    continue\n",
    "                \n",
    "            if extracted_endpoints:\n",
    "                # find the ground truth parameters\n",
    "                pass\n",
    "                # endpoint_result['extracted_parameters'] = {}\n",
    "                # current_endpoint = None\n",
    "                # for endpoint in extracted_endpoints:\n",
    "                #     current_endpoint_name = endpoint['name'].replace(' ','_').lower() + '_' + endpoint['method']\n",
    "                #     if current_endpoint_name == endpoint_name:\n",
    "                #         current_endpoint = endpoint\n",
    "                #         print(f\"Found endpoint {endpoint_name} in extracted information\")\n",
    "                #         break\n",
    "                # if not current_endpoint:\n",
    "                #     print(f\"Endpoint {endpoint_name} not found in extracted information\")\n",
    "                #     continue\n",
    "                # for req_param in current_endpoint['required_parameters']:\n",
    "                #     endpoint_result['extracted_parameters'][req_param['name']] = {'description': req_param['description'], 'example': req_param['example']}\n",
    "                # if current_endpoint['optional_parameters']:\n",
    "                #     for opt_param in current_endpoint['optional_parameters']:\n",
    "                #         endpoint_result['extracted_parameters'][opt_param['name']] = {'description': opt_param['description'], 'example': opt_param['example']}\n",
    "            f = get_function(folder, endpoint_name)\n",
    "            param_names = get_parameter_names(f)\n",
    "            # simulate the situation that no example is provided\n",
    "            # for a single parameter function, we can try a few test cases.\n",
    "            if len(param_names) == 1:\n",
    "                endpoint_result['tests'] = []\n",
    "                curr_param = param_names[0]\n",
    "                examples = get_test_examples(curr_param, folder, max_param_examples=5, max_response_examples=5, max_examples_per_hit=1, similarity_threshold=0.5)\n",
    "                for test_param, example in examples:\n",
    "                    try:\n",
    "                        response = f(example)\n",
    "                    except:\n",
    "                        print(f'Error occured when calling {folder}.{endpoint_name} with {curr_param}={example}')\n",
    "                        continue\n",
    "                    response_json =\"\"\n",
    "                    try:\n",
    "                        response_json = response.json()\n",
    "                    except:\n",
    "                        pass\n",
    "                    example_result = {'gt_param': curr_param,'test_param': test_param, 'candidate': example, 'status_code': response.status_code, 'json': response_json, 'text': response.text}\n",
    "                    endpoint_result['tests'].append(example_result)\n",
    "                    time.sleep(0.1)\n",
    "                pass \n",
    "            else:\n",
    "                # use a rough estimation, record the ranking of the ground truth parameter(?)\n",
    "                endpoint_result['tests'] = []\n",
    "                example_list = []\n",
    "                for param in param_names:\n",
    "                    examples = get_test_examples(param, folder, max_param_examples=5, max_response_examples=5, max_examples_per_hit=1, similarity_threshold=0.5)\n",
    "                    example_list.append(examples)\n",
    "                candidate_combinations = list(itertools.product(*example_list))\n",
    "                samples = random.sample(candidate_combinations, min(MAX_SAMPLES, len(candidate_combinations)))\n",
    "                print(f\"Testing {folder}.{endpoint_name} with {len(samples)} samples\")\n",
    "                for sample in samples:\n",
    "                    try:\n",
    "                        input_dict = {param_names[i]: sample[i][1] for i in range(len(param_names))}\n",
    "                        response = f(**input_dict)\n",
    "                    except:\n",
    "                        print(f'Error occured when calling {folder}.{endpoint_name} with {input_dict}')\n",
    "                        continue\n",
    "                    response_json =\"\"\n",
    "                    try:\n",
    "                        response_json = response.json()\n",
    "                    except:\n",
    "                        pass\n",
    "                    example_result = {'gt_param': param_names,'test_param': [t[0] for t in sample], 'candidate': [t[1] for t in sample], 'status_code': response.status_code, 'json': response_json, 'text': response.text}\n",
    "                    endpoint_result['tests'].append(example_result)\n",
    "                    time.sleep(0.1)\n",
    "                pass\n",
    "            # save endpoint_result\n",
    "            save_path = os.path.join(apidocs_dir, folder, endpoint_name + \"_example_test.json\")\n",
    "            with open(save_path, \"w\") as f:\n",
    "                json.dump(endpoint_result, f)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_experiment()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# let gpt evaluate the tested examples\n",
    "1. check if status code is 200\n",
    "2. chunk the response\n",
    "3. sent response to gpt, let it check if the API returns an actual response rather than an error information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.pydantic_v1 import BaseModel, Field\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "tagging_prompt = ChatPromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "Decide if the following API response is an information or an error message.\n",
    "\n",
    "API Description:\n",
    "{description}\n",
    "API Response:\n",
    "{response}\n",
    "\"\"\"\n",
    ")\n",
    "class Classification(BaseModel):\n",
    "    response_type: str= Field(..., enum=['information', 'error'])\n",
    "llm = ChatOpenAI(temperature=0, model=\"gpt-4o-mini\").with_structured_output(Classification)\n",
    "\n",
    "def gpt_evaluate(API_description:str, API_response: str):\n",
    "    '''let gpt to decide if the API response is a piece of information or an error message'''\n",
    "    prompt = tagging_prompt.invoke({\"description\": API_description, \"response\": API_response})\n",
    "    gpt_response = llm.invoke(prompt)\n",
    "    return gpt_response.response_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "apidocs_dir = \"../extractor/apidocs\"\n",
    "folders = os.listdir(apidocs_dir)\n",
    "count_single_param_files = 0\n",
    "count_multi_param_files = 0\n",
    "count_validated_files = 0\n",
    "count_validated_single_param_files = 0\n",
    "count_validated_multi_param_files = 0\n",
    "count_find_by_scemantic = 0\n",
    "count_find_by_scemantic_single = 0\n",
    "count_find_by_scemantic_multi = 0\n",
    "scemantic_case = []\n",
    "for folder in folders:\n",
    "    files = os.listdir(os.path.join(apidocs_dir, folder))\n",
    "    files = [x for x in files if x.endswith(\"_example_test.json\")]\n",
    "    for file_name in files:\n",
    "        file_path = os.path.join(apidocs_dir, folder, file_name)\n",
    "        # if json file is larger than 10MB, skip it\n",
    "        if os.path.getsize(file_path) > 100*1024*1024:\n",
    "            continue\n",
    "        endpoint_result = json.load(open(file_path))\n",
    "        endpoint_result['validated_parameters'] = {}\n",
    "        print(f\"Validating {folder}.{endpoint_result['endpoint']}\")\n",
    "        saved_error_responses = set() # API responses usually have similar error messages, save them to avoid repeated evaluation\n",
    "        if endpoint_result['tests']:\n",
    "            if type(endpoint_result['tests'][0]['gt_param']) == str:\n",
    "                count_single_param_files += 1\n",
    "            else:\n",
    "                count_multi_param_files += 1\n",
    "        for test in endpoint_result['tests']:\n",
    "            status_code = test['status_code']\n",
    "            if status_code == 200:\n",
    "                response_text = test['text']\n",
    "                # truncate the response text to avoid the max token limit of gpt\n",
    "                response_text = response_text[:500] if len(response_text) > 500 else response_text\n",
    "                if response_text in saved_error_responses:\n",
    "                    continue\n",
    "                else:\n",
    "                    # if type(test['gt_param']) == str:\n",
    "                    #     continue\n",
    "                    gpt_response = gpt_evaluate(endpoint_result['endpoint'],response_text)\n",
    "                    if gpt_response == 'error':\n",
    "                        print(f\"Error response detected: {response_text}\")\n",
    "                        saved_error_responses.add(response_text)\n",
    "                    else:\n",
    "                        print(f\"Information response detected: {response_text}\")\n",
    "                        count_validated_files += 1\n",
    "                        if type(test['gt_param']) == str:\n",
    "                            endpoint_result['validated_parameters'][test['gt_param']] = test['candidate']\n",
    "                            count_validated_single_param_files += 1\n",
    "                            if not test['test_param'] in endpoint_result['extracted_parameters']:\n",
    "                                count_find_by_scemantic += 1\n",
    "                                count_find_by_scemantic_single += 1\n",
    "                                scemantic_case.append((folder, endpoint_result['endpoint'], test['test_param'], test['gt_param']))\n",
    "                        else:\n",
    "                            count_validated_multi_param_files += 1\n",
    "                            for i, gt_param in enumerate(test['gt_param']):\n",
    "                                endpoint_result['validated_parameters'][gt_param] = test['candidate'][i]\n",
    "                                if not test['test_param'][i] in endpoint_result['extracted_parameters']:\n",
    "                                    count_find_by_scemantic += 1\n",
    "                                    count_find_by_scemantic_multi += 1\n",
    "                                    scemantic_case.append((folder, endpoint_result['endpoint'], test['test_param'][i], gt_param))\n",
    "                        break\n",
    "        with open(file_path, \"w\") as f:\n",
    "            json.dump(endpoint_result, f)\n",
    "print(f\"Validated {count_validated_files} parameters in {count_multi_param_files+count_single_param_files} files. {count_find_by_scemantic} parameters are found by semantic search.\") \n",
    "print(f\"Validated {count_validated_single_param_files} single parameters in {count_single_param_files} files. {count_find_by_scemantic_single} single parameters are found by semantic search.\")\n",
    "print(f\"Validated {count_validated_multi_param_files} multi parameters in {count_multi_param_files} files. {count_find_by_scemantic_multi} multi parameters are found by semantic search.\")\n",
    "print(scemantic_case)\n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".conda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
